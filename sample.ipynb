{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 1: Markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "\n",
    "np.random.seed(1234)#to get consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = {} \n",
    "first_order = {} \n",
    "second_order = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_punctuation(s):\n",
    "#     return s.translate(str.maketrans('','',string.punctuation))\n",
    "def remove_punctuation(s):\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add2dict(d, k, v):\n",
    "    if k not in d:\n",
    "        d[k] = []\n",
    "    d[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in open(\"Poems\\Emily Dickinson\\Emily Dickinsons' poems.txt\"):\n",
    "    tokens = remove_punctuation(line.rstrip().lower()).split()\n",
    "\n",
    "    T = len(tokens)\n",
    "    for i in range(T):\n",
    "        t = tokens[i]\n",
    "        if i == 0:\n",
    "            initial[t] = initial.get(t, 0.) + 1\n",
    "        else:\n",
    "            t_1 = tokens[i-1]\n",
    "            if i == T - 1:\n",
    "\n",
    "                add2dict(second_order, (t_1, t), 'END')\n",
    "            if i == 1:\n",
    "\n",
    "                add2dict(first_order, t_1, t)\n",
    "            else:\n",
    "                t_2 = tokens[i-2]\n",
    "                add2dict(second_order, (t_2, t_1), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_total = sum(initial.values())\n",
    "for t, c in initial.items():\n",
    "    initial[t] = c / initial_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2pdict(ts):\n",
    "    d = {}\n",
    "    n = len(ts)\n",
    "    for t in ts:\n",
    "        d[t] = d.get(t, 0.) + 1\n",
    "    for t, c in d.items():\n",
    "        d[t] = c / n\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_1, ts in first_order.items():\n",
    "    first_order[t_1] = list2pdict(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, ts in second_order.items():\n",
    "    second_order[k] = list2pdict(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(d):\n",
    "    p0 = np.random.random()\n",
    "    cumulative = 0\n",
    "    for t, p in d.items():\n",
    "        cumulative += p\n",
    "        if p0 < cumulative:\n",
    "            return t\n",
    "    assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    for i in range(6): \n",
    "        sentence = []\n",
    "\n",
    "        w0 = sample_word(initial)\n",
    "        sentence.append(w0)\n",
    "\n",
    "        w1 = sample_word(first_order[w0])\n",
    "        sentence.append(w1)\n",
    "\n",
    "        while True:\n",
    "            w2 = sample_word(second_order[(w0, w1)])\n",
    "            if w2 == 'END':\n",
    "                break\n",
    "            sentence.append(w2)\n",
    "            w0 = w1\n",
    "            w1 = w2\n",
    "        print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm so accustomed to the heart that broke so long—\n",
      "then thought of us, and return—\n",
      "those boys and girls\n",
      "but internal difference\n",
      "eden—a legend—dimly told—\n",
      "or what circassian land?\n"
     ]
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 2: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 11, 100)           1325500   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 11, 150)           150600    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 13255)             1338755   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,915,255\n",
      "Trainable params: 2,915,255\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1554/1554 [==============================] - 69s 43ms/step - loss: 7.8790 - accuracy: 0.0618\n",
      "Epoch 2/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 7.3343 - accuracy: 0.0666\n",
      "Epoch 3/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 7.0617 - accuracy: 0.0711\n",
      "Epoch 4/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 6.8095 - accuracy: 0.0787\n",
      "Epoch 5/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 6.5722 - accuracy: 0.0870\n",
      "Epoch 6/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 6.3594 - accuracy: 0.0903\n",
      "Epoch 7/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 6.1515 - accuracy: 0.0933\n",
      "Epoch 8/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 5.9565 - accuracy: 0.0993\n",
      "Epoch 9/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 5.7681 - accuracy: 0.1030\n",
      "Epoch 10/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 5.5792 - accuracy: 0.1088\n",
      "Epoch 11/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 5.3926 - accuracy: 0.1154\n",
      "Epoch 12/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 5.2121 - accuracy: 0.1233\n",
      "Epoch 13/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 5.0406 - accuracy: 0.1309\n",
      "Epoch 14/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 4.8749 - accuracy: 0.1399\n",
      "Epoch 15/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 4.7133 - accuracy: 0.1534\n",
      "Epoch 16/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 4.5603 - accuracy: 0.1678\n",
      "Epoch 17/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 4.4178 - accuracy: 0.1841\n",
      "Epoch 18/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 4.2798 - accuracy: 0.2027\n",
      "Epoch 19/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 4.1493 - accuracy: 0.2226\n",
      "Epoch 20/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 4.0263 - accuracy: 0.2397\n",
      "Epoch 21/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 3.9088 - accuracy: 0.2590\n",
      "Epoch 22/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 3.7984 - accuracy: 0.2759\n",
      "Epoch 23/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 3.6944 - accuracy: 0.2926\n",
      "Epoch 24/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 3.5942 - accuracy: 0.3087\n",
      "Epoch 25/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 3.5033 - accuracy: 0.3239\n",
      "Epoch 26/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 3.4128 - accuracy: 0.3413\n",
      "Epoch 27/100\n",
      "1554/1554 [==============================] - 61s 39ms/step - loss: 3.3257 - accuracy: 0.3558\n",
      "Epoch 28/100\n",
      "1554/1554 [==============================] - 61s 39ms/step - loss: 3.2467 - accuracy: 0.3692\n",
      "Epoch 29/100\n",
      "1554/1554 [==============================] - 61s 39ms/step - loss: 3.1681 - accuracy: 0.3836\n",
      "Epoch 30/100\n",
      "1554/1554 [==============================] - 61s 39ms/step - loss: 3.0951 - accuracy: 0.3950\n",
      "Epoch 31/100\n",
      "1554/1554 [==============================] - 61s 39ms/step - loss: 3.0198 - accuracy: 0.4083\n",
      "Epoch 32/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.9541 - accuracy: 0.4201\n",
      "Epoch 33/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 2.8878 - accuracy: 0.4324\n",
      "Epoch 34/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 2.8257 - accuracy: 0.4443\n",
      "Epoch 35/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.7642 - accuracy: 0.4550\n",
      "Epoch 36/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.7052 - accuracy: 0.4645\n",
      "Epoch 37/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.6487 - accuracy: 0.4763\n",
      "Epoch 38/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 2.5963 - accuracy: 0.4844\n",
      "Epoch 39/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.5433 - accuracy: 0.4944\n",
      "Epoch 40/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.4927 - accuracy: 0.5025\n",
      "Epoch 41/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 2.4460 - accuracy: 0.5117\n",
      "Epoch 42/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 2.3996 - accuracy: 0.5202\n",
      "Epoch 43/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.3548 - accuracy: 0.5276\n",
      "Epoch 44/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.3099 - accuracy: 0.5365\n",
      "Epoch 45/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 2.2703 - accuracy: 0.5437\n",
      "Epoch 46/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 2.2313 - accuracy: 0.5515\n",
      "Epoch 47/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.1941 - accuracy: 0.5581\n",
      "Epoch 48/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.1567 - accuracy: 0.5652\n",
      "Epoch 49/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 2.1203 - accuracy: 0.5698\n",
      "Epoch 50/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 2.0870 - accuracy: 0.5776\n",
      "Epoch 51/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.0523 - accuracy: 0.5834\n",
      "Epoch 52/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.0232 - accuracy: 0.5887\n",
      "Epoch 53/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.9926 - accuracy: 0.5939\n",
      "Epoch 54/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.9606 - accuracy: 0.5989\n",
      "Epoch 55/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 1.9324 - accuracy: 0.6050\n",
      "Epoch 56/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.9064 - accuracy: 0.6108\n",
      "Epoch 57/100\n",
      "1554/1554 [==============================] - 61s 40ms/step - loss: 1.8796 - accuracy: 0.6140\n",
      "Epoch 58/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 1.8540 - accuracy: 0.6206\n",
      "Epoch 59/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 1.8302 - accuracy: 0.6243\n",
      "Epoch 60/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 1.8068 - accuracy: 0.6278\n",
      "Epoch 61/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 1.7843 - accuracy: 0.6321\n",
      "Epoch 62/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.7641 - accuracy: 0.6362\n",
      "Epoch 63/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.7409 - accuracy: 0.6411\n",
      "Epoch 64/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.7226 - accuracy: 0.6429\n",
      "Epoch 65/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.7018 - accuracy: 0.6471\n",
      "Epoch 66/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 1.6849 - accuracy: 0.6508\n",
      "Epoch 67/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.6663 - accuracy: 0.6538\n",
      "Epoch 68/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.6501 - accuracy: 0.6576\n",
      "Epoch 69/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.6349 - accuracy: 0.6594\n",
      "Epoch 70/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.6182 - accuracy: 0.6627\n",
      "Epoch 71/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.6038 - accuracy: 0.6640\n",
      "Epoch 72/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.5873 - accuracy: 0.6684\n",
      "Epoch 73/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 1.5753 - accuracy: 0.6701\n",
      "Epoch 74/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.5627 - accuracy: 0.6728\n",
      "Epoch 75/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.5502 - accuracy: 0.6744\n",
      "Epoch 76/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 1.5382 - accuracy: 0.6762\n",
      "Epoch 77/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.5254 - accuracy: 0.6780\n",
      "Epoch 78/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.5155 - accuracy: 0.6811\n",
      "Epoch 79/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.5043 - accuracy: 0.6821\n",
      "Epoch 80/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.4943 - accuracy: 0.6841\n",
      "Epoch 81/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.4843 - accuracy: 0.6847\n",
      "Epoch 82/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.4765 - accuracy: 0.6868\n",
      "Epoch 83/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 1.4672 - accuracy: 0.6873\n",
      "Epoch 84/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.4588 - accuracy: 0.6885\n",
      "Epoch 85/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.4497 - accuracy: 0.6902\n",
      "Epoch 86/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.4416 - accuracy: 0.6918\n",
      "Epoch 87/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.4351 - accuracy: 0.6928\n",
      "Epoch 88/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.4274 - accuracy: 0.6932\n",
      "Epoch 89/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.4211 - accuracy: 0.6934\n",
      "Epoch 90/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.4137 - accuracy: 0.6946\n",
      "Epoch 91/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.4067 - accuracy: 0.6972\n",
      "Epoch 92/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.4023 - accuracy: 0.6967\n",
      "Epoch 93/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.3952 - accuracy: 0.6973\n",
      "Epoch 94/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.3877 - accuracy: 0.6981\n",
      "Epoch 95/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.3845 - accuracy: 0.6980\n",
      "Epoch 96/100\n",
      "1554/1554 [==============================] - 67s 43ms/step - loss: 1.3792 - accuracy: 0.6992\n",
      "Epoch 97/100\n",
      "1554/1554 [==============================] - 68s 44ms/step - loss: 1.3748 - accuracy: 0.6997\n",
      "Epoch 98/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.3687 - accuracy: 0.6996\n",
      "Epoch 99/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.3643 - accuracy: 0.7008\n",
      "Epoch 100/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 1.3612 - accuracy: 0.7009\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "your seed text here numbered more unto the crown our to place— accompany her\n"
     ]
    }
   ],
   "source": [
    "with open(\"Poems\\Emily Dickinson\\Emily Dickinsons' poems.txt\", 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = text.lower()  \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "label = tf.keras.utils.to_categorical(label, num_classes=total_words)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(predictors, label, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "I am alive—because bold so far— suspect me with him the sea of that— by steel he not be ended— – along\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"I am\"\n",
    "next_words = 20  \n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 11, 100)           1325500   \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (None, 11, 150)           150600    \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 200)              200800    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 13255)             2664255   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,341,155\n",
      "Trainable params: 4,341,155\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1554/1554 [==============================] - 97s 60ms/step - loss: 7.8496 - accuracy: 0.0620\n",
      "Epoch 2/10\n",
      "1554/1554 [==============================] - 95s 61ms/step - loss: 7.2793 - accuracy: 0.0686\n",
      "Epoch 3/10\n",
      "1554/1554 [==============================] - 91s 59ms/step - loss: 7.0096 - accuracy: 0.0715\n",
      "Epoch 4/10\n",
      "1554/1554 [==============================] - 90s 58ms/step - loss: 6.7535 - accuracy: 0.0808\n",
      "Epoch 5/10\n",
      "1554/1554 [==============================] - 94s 61ms/step - loss: 6.5072 - accuracy: 0.0875\n",
      "Epoch 6/10\n",
      "1554/1554 [==============================] - 91s 58ms/step - loss: 6.2861 - accuracy: 0.0909\n",
      "Epoch 7/10\n",
      "1554/1554 [==============================] - 95s 61ms/step - loss: 6.0717 - accuracy: 0.0964\n",
      "Epoch 8/10\n",
      "1554/1554 [==============================] - 94s 61ms/step - loss: 5.8501 - accuracy: 0.1007\n",
      "Epoch 9/10\n",
      "1554/1554 [==============================] - 94s 60ms/step - loss: 5.6130 - accuracy: 0.1066\n",
      "Epoch 10/10\n",
      "1554/1554 [==============================] - 94s 61ms/step - loss: 5.3884 - accuracy: 0.1136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d750caba30>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"Poems\\Emily Dickinson\\Emily Dickinsons' poems.txt\", 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = text.lower()  \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "label = tf.keras.utils.to_categorical(label, num_classes=total_words)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(predictors, label, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Summer is\"\n",
    "next_words = 20  \n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summer is the sun of the sun of the sun air come— swain i come— come— swain swain swain one santa workman—\n"
     ]
    }
   ],
   "source": [
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "the sun of the sun\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "of the sun air come—\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "swain i come— come— swain\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "swain swain one santa workman—\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "workman— santa santa workman— workman—\n"
     ]
    }
   ],
   "source": [
    "# Generate poetry in multiple lines\n",
    "seed_text = \"Summer is\"\n",
    "next_lines = 5 \n",
    "words_per_line = 5  \n",
    "for _ in range(next_lines):\n",
    "    generated_words = []\n",
    "    for _ in range(words_per_line):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "        generated_words.append(output_word)\n",
    "    generated_line = \" \".join(generated_words)\n",
    "    print(generated_line)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM without punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 11, 100)           1321000   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 11, 150)           150600    \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 200)              200800    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 13210)             2655210   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,327,610\n",
      "Trainable params: 4,327,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      " 258/1547 [====>.........................] - ETA: 1:17 - loss: 8.1571 - accuracy: 0.0624"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     43\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[1;32m---> 45\u001b[0m model\u001b[39m.\u001b[39;49mfit(predictors, label, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     47\u001b[0m seed_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mI am\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m next_words \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m  \n",
      "File \u001b[1;32mc:\\Users\\lmbmo\\Documents\\Personal\\Projects\\Poetry-generator\\vevn\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\lmbmo\\Documents\\Personal\\Projects\\Poetry-generator\\vevn\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\lmbmo\\Documents\\Personal\\Projects\\Poetry-generator\\vevn\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\lmbmo\\Documents\\Personal\\Projects\\Poetry-generator\\vevn\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\lmbmo\\Documents\\Personal\\Projects\\Poetry-generator\\vevn\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\lmbmo\\Documents\\Personal\\Projects\\Poetry-generator\\vevn\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\lmbmo\\Documents\\Personal\\Projects\\Poetry-generator\\vevn\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\lmbmo\\Documents\\Personal\\Projects\\Poetry-generator\\vevn\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\lmbmo\\Documents\\Personal\\Projects\\Poetry-generator\\vevn\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import string\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "# Read the text file\n",
    "with open(\"Poems\\Emily Dickinson\\Emily Dickinsons' poems.txt\", 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Convert text to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "label = tf.keras.utils.to_categorical(label, num_classes=total_words)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(predictors, label, epochs=10, verbose=1)\n",
    "\n",
    "seed_text = \"I am\"\n",
    "next_words = 20  \n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "    if output_word == '\\n':\n",
    "        print(seed_text.strip())\n",
    "        seed_text = \"\"\n",
    "\n",
    "print(seed_text.strip())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Poems\\Emily Dickinson\\Emily Dickinsons' poems.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "Exception encountered when calling layer \"bahdanau_attention_1\" (type BahdanauAttention).\n\nin user code:\n\n    File \"C:\\Users\\lmbmo\\AppData\\Local\\Temp\\ipykernel_7876\\4115574913.py\", line 10, in call  *\n        query, value = inputs\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\n\n\nCall arguments received by layer \"bahdanau_attention_1\" (type BahdanauAttention):\n  • inputs=tf.Tensor(shape=(None, 10, 256), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m model\u001b[39m.\u001b[39madd(Embedding(vocab_size, embedding_dim, input_length\u001b[39m=\u001b[39msequence_length))\n\u001b[0;32m     56\u001b[0m model\u001b[39m.\u001b[39madd(LSTM(hidden_units, return_sequences\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m---> 57\u001b[0m model\u001b[39m.\u001b[39;49madd(BahdanauAttention(hidden_units))\n\u001b[0;32m     58\u001b[0m model\u001b[39m.\u001b[39madd(Dense(vocab_size, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     60\u001b[0m \u001b[39m# Compile the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lmbmo\\Documents\\Personal\\Projects\\Poetry-generator\\vevn\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lmbmo\\Documents\\Personal\\Projects\\Poetry-generator\\vevn\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\lmbmo\\Documents\\Personal\\Projects\\Poetry-generator\\vevn\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 692\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m    693\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    694\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m: Exception encountered when calling layer \"bahdanau_attention_1\" (type BahdanauAttention).\n\nin user code:\n\n    File \"C:\\Users\\lmbmo\\AppData\\Local\\Temp\\ipykernel_7876\\4115574913.py\", line 10, in call  *\n        query, value = inputs\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\n\n\nCall arguments received by layer \"bahdanau_attention_1\" (type BahdanauAttention):\n  • inputs=tf.Tensor(shape=(None, 10, 256), dtype=float32)"
     ]
    }
   ],
   "source": [
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, value = inputs\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(value)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * value\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "with open(\"Poems\\Emily Dickinson\\Emily Dickinsons' poems.txt\", 'r') as file:\n",
    "    poems_text = file.read()\n",
    "\n",
    "poems_text = poems_text.lower()\n",
    "poems_text = poems_text.replace('\\n', ' \\n ')\n",
    "poems_text = poems_text.replace('\\r', ' ')\n",
    "poems_text = ' '.join(poems_text.split())\n",
    "\n",
    "# Creating the vocabulary\n",
    "words = poems_text.split()\n",
    "word_to_index = {word: i for i, word in enumerate(set(words))}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "# Generate input-output pairs\n",
    "sequences = []\n",
    "next_words = []\n",
    "sequence_length = 10\n",
    "\n",
    "for i in range(len(words) - sequence_length):\n",
    "    sequence = words[i:i+sequence_length]\n",
    "    target = words[i+sequence_length]\n",
    "    sequences.append([word_to_index[word] for word in sequence])\n",
    "    next_words.append(word_to_index[target])\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "next_words = np.array(next_words)\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_units = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=sequence_length))\n",
    "model.add(LSTM(hidden_units, return_sequences=True))\n",
    "model.add(BahdanauAttention(hidden_units))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(sequences, next_words, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sequence = \"i\"\n",
    "generated_poem = [start_sequence]\n",
    "num_lines = 10\n",
    "\n",
    "for _ in range(num_lines):\n",
    "    encoded_input = [word_to_index[word] for word in generated_poem]\n",
    "    encoded_input = pad_sequences([encoded_input], maxlen=sequence_length)\n",
    "    predicted_index = np.argmax(model.predict(encoded_input))\n",
    "    predicted_word = index_to_word[predicted_index]\n",
    "    generated_poem.append(predicted_word)\n",
    "\n",
    "for line in generated_poem:\n",
    "    print(line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
