{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 1: Markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "\n",
    "np.random.seed(1234)#to get consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = {} \n",
    "first_order = {} \n",
    "second_order = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_punctuation(s):\n",
    "#     return s.translate(str.maketrans('','',string.punctuation))\n",
    "def remove_punctuation(s):\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add2dict(d, k, v):\n",
    "    if k not in d:\n",
    "        d[k] = []\n",
    "    d[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in open(\"Poems\\Emily Dickinson\\Emily Dickinsons' poems.txt\"):\n",
    "    tokens = remove_punctuation(line.rstrip().lower()).split()\n",
    "\n",
    "    T = len(tokens)\n",
    "    for i in range(T):\n",
    "        t = tokens[i]\n",
    "        if i == 0:\n",
    "            initial[t] = initial.get(t, 0.) + 1\n",
    "        else:\n",
    "            t_1 = tokens[i-1]\n",
    "            if i == T - 1:\n",
    "\n",
    "                add2dict(second_order, (t_1, t), 'END')\n",
    "            if i == 1:\n",
    "\n",
    "                add2dict(first_order, t_1, t)\n",
    "            else:\n",
    "                t_2 = tokens[i-2]\n",
    "                add2dict(second_order, (t_2, t_1), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_total = sum(initial.values())\n",
    "for t, c in initial.items():\n",
    "    initial[t] = c / initial_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2pdict(ts):\n",
    "    d = {}\n",
    "    n = len(ts)\n",
    "    for t in ts:\n",
    "        d[t] = d.get(t, 0.) + 1\n",
    "    for t, c in d.items():\n",
    "        d[t] = c / n\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_1, ts in first_order.items():\n",
    "    first_order[t_1] = list2pdict(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, ts in second_order.items():\n",
    "    second_order[k] = list2pdict(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(d):\n",
    "    p0 = np.random.random()\n",
    "    cumulative = 0\n",
    "    for t, p in d.items():\n",
    "        cumulative += p\n",
    "        if p0 < cumulative:\n",
    "            return t\n",
    "    assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    for i in range(6): \n",
    "        sentence = []\n",
    "\n",
    "        w0 = sample_word(initial)\n",
    "        sentence.append(w0)\n",
    "\n",
    "        w1 = sample_word(first_order[w0])\n",
    "        sentence.append(w1)\n",
    "\n",
    "        while True:\n",
    "            w2 = sample_word(second_order[(w0, w1)])\n",
    "            if w2 == 'END':\n",
    "                break\n",
    "            sentence.append(w2)\n",
    "            w0 = w1\n",
    "            w1 = w2\n",
    "        print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm so accustomed to the heart that broke so long—\n",
      "then thought of us, and return—\n",
      "those boys and girls\n",
      "but internal difference\n",
      "eden—a legend—dimly told—\n",
      "or what circassian land?\n"
     ]
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 2: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 11, 100)           1325500   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 11, 150)           150600    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 13255)             1338755   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,915,255\n",
      "Trainable params: 2,915,255\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1554/1554 [==============================] - 69s 43ms/step - loss: 7.8790 - accuracy: 0.0618\n",
      "Epoch 2/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 7.3343 - accuracy: 0.0666\n",
      "Epoch 3/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 7.0617 - accuracy: 0.0711\n",
      "Epoch 4/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 6.8095 - accuracy: 0.0787\n",
      "Epoch 5/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 6.5722 - accuracy: 0.0870\n",
      "Epoch 6/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 6.3594 - accuracy: 0.0903\n",
      "Epoch 7/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 6.1515 - accuracy: 0.0933\n",
      "Epoch 8/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 5.9565 - accuracy: 0.0993\n",
      "Epoch 9/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 5.7681 - accuracy: 0.1030\n",
      "Epoch 10/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 5.5792 - accuracy: 0.1088\n",
      "Epoch 11/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 5.3926 - accuracy: 0.1154\n",
      "Epoch 12/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 5.2121 - accuracy: 0.1233\n",
      "Epoch 13/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 5.0406 - accuracy: 0.1309\n",
      "Epoch 14/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 4.8749 - accuracy: 0.1399\n",
      "Epoch 15/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 4.7133 - accuracy: 0.1534\n",
      "Epoch 16/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 4.5603 - accuracy: 0.1678\n",
      "Epoch 17/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 4.4178 - accuracy: 0.1841\n",
      "Epoch 18/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 4.2798 - accuracy: 0.2027\n",
      "Epoch 19/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 4.1493 - accuracy: 0.2226\n",
      "Epoch 20/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 4.0263 - accuracy: 0.2397\n",
      "Epoch 21/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 3.9088 - accuracy: 0.2590\n",
      "Epoch 22/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 3.7984 - accuracy: 0.2759\n",
      "Epoch 23/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 3.6944 - accuracy: 0.2926\n",
      "Epoch 24/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 3.5942 - accuracy: 0.3087\n",
      "Epoch 25/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 3.5033 - accuracy: 0.3239\n",
      "Epoch 26/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 3.4128 - accuracy: 0.3413\n",
      "Epoch 27/100\n",
      "1554/1554 [==============================] - 61s 39ms/step - loss: 3.3257 - accuracy: 0.3558\n",
      "Epoch 28/100\n",
      "1554/1554 [==============================] - 61s 39ms/step - loss: 3.2467 - accuracy: 0.3692\n",
      "Epoch 29/100\n",
      "1554/1554 [==============================] - 61s 39ms/step - loss: 3.1681 - accuracy: 0.3836\n",
      "Epoch 30/100\n",
      "1554/1554 [==============================] - 61s 39ms/step - loss: 3.0951 - accuracy: 0.3950\n",
      "Epoch 31/100\n",
      "1554/1554 [==============================] - 61s 39ms/step - loss: 3.0198 - accuracy: 0.4083\n",
      "Epoch 32/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.9541 - accuracy: 0.4201\n",
      "Epoch 33/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 2.8878 - accuracy: 0.4324\n",
      "Epoch 34/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 2.8257 - accuracy: 0.4443\n",
      "Epoch 35/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.7642 - accuracy: 0.4550\n",
      "Epoch 36/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.7052 - accuracy: 0.4645\n",
      "Epoch 37/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.6487 - accuracy: 0.4763\n",
      "Epoch 38/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 2.5963 - accuracy: 0.4844\n",
      "Epoch 39/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.5433 - accuracy: 0.4944\n",
      "Epoch 40/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.4927 - accuracy: 0.5025\n",
      "Epoch 41/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 2.4460 - accuracy: 0.5117\n",
      "Epoch 42/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 2.3996 - accuracy: 0.5202\n",
      "Epoch 43/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.3548 - accuracy: 0.5276\n",
      "Epoch 44/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.3099 - accuracy: 0.5365\n",
      "Epoch 45/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 2.2703 - accuracy: 0.5437\n",
      "Epoch 46/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 2.2313 - accuracy: 0.5515\n",
      "Epoch 47/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.1941 - accuracy: 0.5581\n",
      "Epoch 48/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.1567 - accuracy: 0.5652\n",
      "Epoch 49/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 2.1203 - accuracy: 0.5698\n",
      "Epoch 50/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 2.0870 - accuracy: 0.5776\n",
      "Epoch 51/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.0523 - accuracy: 0.5834\n",
      "Epoch 52/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 2.0232 - accuracy: 0.5887\n",
      "Epoch 53/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.9926 - accuracy: 0.5939\n",
      "Epoch 54/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.9606 - accuracy: 0.5989\n",
      "Epoch 55/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 1.9324 - accuracy: 0.6050\n",
      "Epoch 56/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.9064 - accuracy: 0.6108\n",
      "Epoch 57/100\n",
      "1554/1554 [==============================] - 61s 40ms/step - loss: 1.8796 - accuracy: 0.6140\n",
      "Epoch 58/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 1.8540 - accuracy: 0.6206\n",
      "Epoch 59/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 1.8302 - accuracy: 0.6243\n",
      "Epoch 60/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 1.8068 - accuracy: 0.6278\n",
      "Epoch 61/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 1.7843 - accuracy: 0.6321\n",
      "Epoch 62/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.7641 - accuracy: 0.6362\n",
      "Epoch 63/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.7409 - accuracy: 0.6411\n",
      "Epoch 64/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.7226 - accuracy: 0.6429\n",
      "Epoch 65/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.7018 - accuracy: 0.6471\n",
      "Epoch 66/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 1.6849 - accuracy: 0.6508\n",
      "Epoch 67/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.6663 - accuracy: 0.6538\n",
      "Epoch 68/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.6501 - accuracy: 0.6576\n",
      "Epoch 69/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.6349 - accuracy: 0.6594\n",
      "Epoch 70/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.6182 - accuracy: 0.6627\n",
      "Epoch 71/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.6038 - accuracy: 0.6640\n",
      "Epoch 72/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.5873 - accuracy: 0.6684\n",
      "Epoch 73/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 1.5753 - accuracy: 0.6701\n",
      "Epoch 74/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.5627 - accuracy: 0.6728\n",
      "Epoch 75/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.5502 - accuracy: 0.6744\n",
      "Epoch 76/100\n",
      "1554/1554 [==============================] - 62s 40ms/step - loss: 1.5382 - accuracy: 0.6762\n",
      "Epoch 77/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.5254 - accuracy: 0.6780\n",
      "Epoch 78/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.5155 - accuracy: 0.6811\n",
      "Epoch 79/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.5043 - accuracy: 0.6821\n",
      "Epoch 80/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.4943 - accuracy: 0.6841\n",
      "Epoch 81/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.4843 - accuracy: 0.6847\n",
      "Epoch 82/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.4765 - accuracy: 0.6868\n",
      "Epoch 83/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 1.4672 - accuracy: 0.6873\n",
      "Epoch 84/100\n",
      "1554/1554 [==============================] - 63s 40ms/step - loss: 1.4588 - accuracy: 0.6885\n",
      "Epoch 85/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.4497 - accuracy: 0.6902\n",
      "Epoch 86/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.4416 - accuracy: 0.6918\n",
      "Epoch 87/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.4351 - accuracy: 0.6928\n",
      "Epoch 88/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.4274 - accuracy: 0.6932\n",
      "Epoch 89/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.4211 - accuracy: 0.6934\n",
      "Epoch 90/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.4137 - accuracy: 0.6946\n",
      "Epoch 91/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.4067 - accuracy: 0.6972\n",
      "Epoch 92/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.4023 - accuracy: 0.6967\n",
      "Epoch 93/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.3952 - accuracy: 0.6973\n",
      "Epoch 94/100\n",
      "1554/1554 [==============================] - 64s 41ms/step - loss: 1.3877 - accuracy: 0.6981\n",
      "Epoch 95/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.3845 - accuracy: 0.6980\n",
      "Epoch 96/100\n",
      "1554/1554 [==============================] - 67s 43ms/step - loss: 1.3792 - accuracy: 0.6992\n",
      "Epoch 97/100\n",
      "1554/1554 [==============================] - 68s 44ms/step - loss: 1.3748 - accuracy: 0.6997\n",
      "Epoch 98/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.3687 - accuracy: 0.6996\n",
      "Epoch 99/100\n",
      "1554/1554 [==============================] - 65s 42ms/step - loss: 1.3643 - accuracy: 0.7008\n",
      "Epoch 100/100\n",
      "1554/1554 [==============================] - 63s 41ms/step - loss: 1.3612 - accuracy: 0.7009\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "your seed text here numbered more unto the crown our to place— accompany her\n"
     ]
    }
   ],
   "source": [
    "with open(\"Poems\\Emily Dickinson\\Emily Dickinsons' poems.txt\", 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = text.lower()  \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "label = tf.keras.utils.to_categorical(label, num_classes=total_words)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(predictors, label, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "I am alive—because bold so far— suspect me with him the sea of that— by steel he not be ended— – along\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"I am\"\n",
    "next_words = 20  \n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate poetry in multiple lines\n",
    "seed_text = \"Summer is\"\n",
    "next_lines = 5 \n",
    "words_per_line = 5  \n",
    "for _ in range(next_lines):\n",
    "    generated_words = []\n",
    "    for _ in range(words_per_line):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "        generated_words.append(output_word)\n",
    "    generated_line = \" \".join(generated_words)\n",
    "    print(generated_line)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Poems\\Emily Dickinson\\Emily Dickinsons' poems.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "Exception encountered when calling layer \"bahdanau_attention_1\" (type BahdanauAttention).\n\nin user code:\n\n    File \"C:\\Users\\lmbmo\\AppData\\Local\\Temp\\ipykernel_7876\\4115574913.py\", line 10, in call  *\n        query, value = inputs\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\n\n\nCall arguments received by layer \"bahdanau_attention_1\" (type BahdanauAttention):\n  • inputs=tf.Tensor(shape=(None, 10, 256), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m model\u001b[39m.\u001b[39madd(Embedding(vocab_size, embedding_dim, input_length\u001b[39m=\u001b[39msequence_length))\n\u001b[0;32m     56\u001b[0m model\u001b[39m.\u001b[39madd(LSTM(hidden_units, return_sequences\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m---> 57\u001b[0m model\u001b[39m.\u001b[39;49madd(BahdanauAttention(hidden_units))\n\u001b[0;32m     58\u001b[0m model\u001b[39m.\u001b[39madd(Dense(vocab_size, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     60\u001b[0m \u001b[39m# Compile the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lmbmo\\Documents\\Personal\\Projects\\Poetry-generator\\vevn\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lmbmo\\Documents\\Personal\\Projects\\Poetry-generator\\vevn\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\lmbmo\\Documents\\Personal\\Projects\\Poetry-generator\\vevn\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 692\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m    693\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    694\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m: Exception encountered when calling layer \"bahdanau_attention_1\" (type BahdanauAttention).\n\nin user code:\n\n    File \"C:\\Users\\lmbmo\\AppData\\Local\\Temp\\ipykernel_7876\\4115574913.py\", line 10, in call  *\n        query, value = inputs\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\n\n\nCall arguments received by layer \"bahdanau_attention_1\" (type BahdanauAttention):\n  • inputs=tf.Tensor(shape=(None, 10, 256), dtype=float32)"
     ]
    }
   ],
   "source": [
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, value = inputs\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(value)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * value\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "with open(\"Poems\\Emily Dickinson\\Emily Dickinsons' poems.txt\", 'r') as file:\n",
    "    poems_text = file.read()\n",
    "\n",
    "poems_text = poems_text.lower()\n",
    "poems_text = poems_text.replace('\\n', ' \\n ')\n",
    "poems_text = poems_text.replace('\\r', ' ')\n",
    "poems_text = ' '.join(poems_text.split())\n",
    "\n",
    "# Creating the vocabulary\n",
    "words = poems_text.split()\n",
    "word_to_index = {word: i for i, word in enumerate(set(words))}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "# Generate input-output pairs\n",
    "sequences = []\n",
    "next_words = []\n",
    "sequence_length = 10\n",
    "\n",
    "for i in range(len(words) - sequence_length):\n",
    "    sequence = words[i:i+sequence_length]\n",
    "    target = words[i+sequence_length]\n",
    "    sequences.append([word_to_index[word] for word in sequence])\n",
    "    next_words.append(word_to_index[target])\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "next_words = np.array(next_words)\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_units = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=sequence_length))\n",
    "model.add(LSTM(hidden_units, return_sequences=True))\n",
    "model.add(BahdanauAttention(hidden_units))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(sequences, next_words, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sequence = \"i\"\n",
    "generated_poem = [start_sequence]\n",
    "num_lines = 10\n",
    "\n",
    "for _ in range(num_lines):\n",
    "    encoded_input = [word_to_index[word] for word in generated_poem]\n",
    "    encoded_input = pad_sequences([encoded_input], maxlen=sequence_length)\n",
    "    predicted_index = np.argmax(model.predict(encoded_input))\n",
    "    predicted_word = index_to_word[predicted_index]\n",
    "    generated_poem.append(predicted_word)\n",
    "\n",
    "for line in generated_poem:\n",
    "    print(line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
